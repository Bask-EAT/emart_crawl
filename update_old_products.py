import firebase_admin
from firebase_admin import credentials, firestore
from google.cloud.firestore_v1.base_query import FieldFilter
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import time
from typing import Dict, Union, List
import random

# ==============================================================================
# 1. Firebase ì—°ë™ ë° ìŠ¤í¬ë˜í•‘ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================================================


def initialize_firebase():
    """Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if not firebase_admin._apps:
        try:
            cred = credentials.Certificate("repository/serviceAccountKey.json")
            firebase_admin.initialize_app(cred)
            print("âœ… Firebase Admin SDKê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ğŸ”¥ Firebase ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def scrape_single_product(product_id: str, retry_count=0) -> Union[Dict, None]:
    """[ìˆ˜ì •ë¨] í’ˆì ˆ ì‹œ "Y" ë¬¸ìì—´ ëŒ€ì‹  out_of_stock í‚¤ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
    url = f"https://emart.ssg.com/item/itemView.ssg?itemId={product_id}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        out_of_stock = "Y" if "í’ˆì ˆ" in str(soup.select_one(".cdtl_btn_wrap3")) else "N"

        if out_of_stock == "Y":
            # í’ˆì ˆì´ì–´ë„ ì¼ê´€ëœ ë°ì´í„° í˜•íƒœë¥¼ ë°˜í™˜
            return {"out_of_stock": "Y"}

        selling_price_tag = soup.select_one("span.cdtl_new_price.notranslate > em")
        selling_price = (
            selling_price_tag.get_text(strip=True).replace(",", "").replace("ì›", "")
            if selling_price_tag
            else None
        )
        original_price_tag = soup.select_one("span.cdtl_old_price > em")
        if not original_price_tag:
            original_price_tag = soup.select_one("span.cdtl_first_price > em")
        original_price = (
            original_price_tag.get_text(strip=True).replace(",", "").replace("ì›", "")
            if original_price_tag
            else None
        )
        if original_price and not selling_price:
            selling_price = original_price
        elif selling_price and not original_price:
            original_price = selling_price
        elif not original_price and not selling_price:
            price_tag = soup.select_one(".cdtl_row_price em.ssg_price")
            price = (
                price_tag.get_text(strip=True).replace(",", "").replace("ì›", "")
                if price_tag
                else "0"
            )
            original_price, selling_price = price, price
        quantity_tag = soup.select_one("div.cdtl_optprice_wrap > p.cdtl_txt_info")
        quantity = (
            " ".join(quantity_tag.get_text(strip=True).split()) if quantity_tag else ""
        )

        return {
            "id": product_id,
            "original_price": original_price,
            "selling_price": selling_price,
            "quantity": quantity,
            "out_of_stock": out_of_stock,
            "last_updated": datetime.now().isoformat(),
        }
    except requests.exceptions.HTTPError as http_err:
        if http_err.response.status_code == 429:
            if retry_count < 10:
                wait_time = 10 + random.uniform(0, 5)
                print(
                    f"  -> â³ 429 ì—ëŸ¬: {int(wait_time)}ì´ˆ í›„ ì¬ì‹œë„... ({retry_count+1}/10)"
                )
                time.sleep(wait_time)
                return scrape_single_product(product_id, retry_count + 1)
            else:
                print(f"  -> ğŸš¨ ì˜¤ë¥˜: ID {product_id} ì¬ì‹œë„ ì‹¤íŒ¨.")
                return None
        else:
            print(
                f"  -> ğŸš¨ ì˜¤ë¥˜: ID {product_id} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ (HTTP ì—ëŸ¬ {http_err.response.status_code})"
            )
            return None
    except Exception as e:
        print(f"  -> ğŸš¨ ì˜¤ë¥˜: ID {product_id} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        return None


# ==============================================================================
# 3. ë©”ì¸ ë¡œì§ (í•µì‹¬ ìˆ˜ì •)
# ==============================================================================


def find_and_update_stale_products():
    """Firestore ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ê°€ í•˜ë£¨ ì´ìƒ ì§€ë‚œ ìƒí’ˆë§Œ ì°¾ì•„ ê°±ì‹ í•©ë‹ˆë‹¤."""
    # ... (ë‚´ìš© ë™ì¼)
    try:
        initialize_firebase()
        db = firestore.client()
        one_day_ago_iso = (datetime.now() - timedelta(days=7)).isoformat()
        print(f"ğŸš€ ê¸°ì¤€ ì‹œê°„: {one_day_ago_iso} ì´ì „ì— ì—…ë°ì´íŠ¸ëœ ìƒí’ˆì„ ì°¾ìŠµë‹ˆë‹¤.\n")
        product_collection_ref = db.collection("emart_product")
        query = product_collection_ref.where(
            filter=FieldFilter("last_updated", "<", one_day_ago_iso)
        )
        docs_to_update = list(query.stream())
        if not docs_to_update:
            print("âœ… ëª¨ë“  ìƒí’ˆì´ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
            return

        # [ìˆ˜ì •] IDë¿ë§Œ ì•„ë‹ˆë¼ ê¸°ì¡´ ë°ì´í„°ë„ í•¨ê»˜ ì „ë‹¬í•˜ì—¬ DB ì½ê¸° ìµœì†Œí™”
        stale_products = {doc.id: doc.to_dict() for doc in docs_to_update}
        print(
            f"ğŸ” ì´ {len(stale_products)}ê°œì˜ ì˜¤ë˜ëœ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n"
        )
        scrape_and_update_products_by_ids(stale_products)
    except Exception as e:
        print(f"\nğŸ”¥ ì‘ì—… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def delete_product_from_all_collections(product_ids: List[str]):
    """ì£¼ì–´ì§„ ID ëª©ë¡ì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆ ë¬¸ì„œë¥¼ emart_price, emart_product, emart_vectorì—ì„œ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤."""
    # ... (ë‚´ìš© ë™ì¼)
    try:
        initialize_firebase()
        db = firestore.client()
        batch = db.batch()
        for pid in product_ids:
            batch.delete(db.collection("emart_price").document(pid))
            batch.delete(db.collection("emart_product").document(pid))
            batch.delete(db.collection("emart_vector").document(pid))
        batch.commit()
        print(
            f"\nâœ¨ {len(product_ids)}ê°œ IDì— ëŒ€í•œ ë¬¸ì„œ ì‚­ì œ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        )
    except Exception as e:
        print(f"\nğŸ”¥ ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def scrape_and_update_products_by_ids(stale_products: Dict[str, Dict]):
    """
    [ìˆ˜ì •ë¨] ì£¼ì–´ì§„ ìƒí’ˆ ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê³ , ëª¨ë“  DB ì—…ë°ì´íŠ¸ë¥¼ Batchë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    db = firestore.client()
    price_collection_ref = db.collection("emart_price")
    product_collection_ref = db.collection("emart_product")

    batch = db.batch()
    updated_count = 0
    deleted_count = 0

    product_ids = list(stale_products.keys())

    for i, product_id in enumerate(product_ids):
        print(f"({i+1}/{len(product_ids)}) ID: {product_id} ì²˜ë¦¬ ì¤‘...")

        scraped_data = scrape_single_product(product_id)
        if not scraped_data:
            continue

        if scraped_data.get("out_of_stock") == "Y":
            # [ìˆ˜ì •] ì¹˜ëª…ì  ì˜¤ë¥˜ í•´ê²°
            delete_product_from_all_collections([product_id])
            deleted_count += 1
            print(f"  -> ID: {product_id} í’ˆì ˆë¡œ ê°„ì£¼ë˜ì–´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            continue

        # --- [í•µì‹¬ ìˆ˜ì •] DB ì½ê¸° ì—†ì´ Batch ì‘ì—…ë§Œ ìˆ˜í–‰ ---
        price_doc_ref = price_collection_ref.document(product_id)

        price_update_payload = {
            "id": product_id,
            "out_of_stock": scraped_data["out_of_stock"],
            "quantity": scraped_data["quantity"],
            "last_updated": scraped_data["last_updated"],
            "price_history": firestore.ArrayUnion(
                [
                    {
                        "original_price": scraped_data["original_price"],
                        "selling_price": scraped_data["selling_price"],
                        "last_updated": scraped_data["last_updated"],
                    }
                ]
            ),
        }

        batch.set(price_doc_ref, price_update_payload, merge=True)
        product_doc_ref = product_collection_ref.document(product_id)
        batch.update(product_doc_ref, {"last_updated": scraped_data["last_updated"]})

        updated_count += 1
        if updated_count > 0 and updated_count % 50 == 0:
            batch.commit()
            batch = db.batch()

        time.sleep(random.uniform(1, 3))

    if updated_count > 0:
        batch.commit()

    print(f"\nâœ¨ ì´ {updated_count}ê°œ ìƒí’ˆ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°±ì‹ í–ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ—‘ï¸ ì´ {deleted_count}ê°œ ìƒí’ˆì„ í’ˆì ˆ ì²˜ë¦¬ í›„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    find_and_update_stale_products()
